{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)  # For reproducibility\n",
    "n_series = 4000   # Number of time series per class\n",
    "n_points = 500    # Number of data points in each time series\n",
    "\n",
    "#output_dir = 'time_series_data'\n",
    "#os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.expanduser(\"~/timeseries_data\")  # Creates the directory in your home folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR, MA and ARMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate AR, MA, or ARMA data with optional trend and seasonality\n",
    "def generate_time_series(model_type, order, n_points, trend_strength=0.1, seasonality_amplitude=0.5, seasonality_period=50, include_trend=True):\n",
    "   if model_type == 'AR':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -params]\n",
    "       ma = np.array([1])\n",
    "   elif model_type == 'MA':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.array([1])\n",
    "       ma = np.r_[1, params]\n",
    "   elif model_type == 'ARMA':\n",
    "       ar_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ma_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -ar_params]\n",
    "       ma = np.r_[1, ma_params]\n",
    "   else:\n",
    "       raise ValueError(\"Invalid model type. Use 'AR', 'MA', or 'ARMA'.\")\n",
    "   \n",
    "   \n",
    "      # Generate the process\n",
    "   process = sm.tsa.ArmaProcess(ar, ma)\n",
    "   data = process.generate_sample(nsample=n_points)\n",
    "   if include_trend:\n",
    "       trend = np.linspace(0, trend_strength * n_points, n_points)\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += trend + seasonality\n",
    "   else:\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += seasonality\n",
    "   return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for orders 1-3 with and without trend (Kernel crashed after 14 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loop to generate and save time series plots for each class\n",
    "model_types = ['AR', 'MA', 'ARMA']\n",
    "orders = [1, 2, 3]\n",
    "for model_type in model_types:\n",
    "   for order in orders:\n",
    "       for include_trend in [True, False]:\n",
    "           class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "           class_dir = os.path.join(output_dir, class_label)\n",
    "           os.makedirs(class_dir, exist_ok=True)\n",
    "           for i in range(n_series):\n",
    "               data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "               # Plotting the time series\n",
    "               plt.figure(figsize=(8, 4))\n",
    "               plt.plot(data)\n",
    "               plt.axis('off')  # Turn off axes for a clean image\n",
    "               plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "               plt.close()\n",
    "print(\"Time series generation completed. Time series are saved in the 'time_series_data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating image for remaining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMA_1_without_trend: 4000 images\n",
      "ARMA_2_with_trend: 4000 images\n",
      "MA_3_with_trend: 4000 images\n",
      "ARMA_2_without_trend: 4000 images\n",
      "AR_3_without_trend: 4000 images\n",
      "MA_1_without_trend: 4000 images\n",
      "MA_1_with_trend: 4000 images\n",
      "AR_2_with_trend: 4000 images\n",
      "MA_2_without_trend: 4000 images\n",
      "AR_1_with_trend: 4000 images\n",
      "MA_2_with_trend: 4000 images\n",
      "MA_3_without_trend: 4000 images\n",
      "ARMA_3_with_trend: 4000 images\n",
      "AR_3_with_trend: 4000 images\n",
      "AR_2_without_trend: 4000 images\n",
      "ARMA_3_without_trend: 4000 images\n",
      "AR_1_without_trend: 4000 images\n",
      "ARMA_1_with_trend: 4000 images\n"
     ]
    }
   ],
   "source": [
    "# Adjusted classes for ARMA 2 and 3 with and without trend\n",
    "model_type = 'ARMA'\n",
    "orders = [2, 3]\n",
    "for order in orders:\n",
    "    for include_trend in [True, False]:\n",
    "        class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "        class_dir = os.path.join(output_dir, class_label)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for i in range(n_series):\n",
    "            data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "            # Plotting the time series\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(data)\n",
    "            plt.axis('off')  # Turn off axes for a clean image\n",
    "            plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "\n",
    "# Code to check the number of images in each folder\n",
    "folder_status = {}\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_status[folder_name] = len(os.listdir(folder_path))\n",
    "\n",
    "# Display folder status\n",
    "for class_label, image_count in folder_status.items():\n",
    "    print(f\"{class_label}: {image_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Global parameters\n",
    "image_size = (64, 64)  # Resize all images to this size\n",
    "batch_size = 32  # Adjust based on hardware capacity\n",
    "\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")  # Path to your main directory with class subfolders\n",
    "\n",
    "\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])  # Filter directories only\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # Mapping class names to indices\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):  # Ensure it's an image file\n",
    "                img = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
    "                img = img.resize(image_size)  # Resize image\n",
    "                data.append(np.array(img))\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index mapping: {'ARMA_1_with_trend': 0, 'ARMA_1_without_trend': 1, 'ARMA_2_with_trend': 2, 'ARMA_2_without_trend': 3, 'ARMA_3_with_trend': 4, 'ARMA_3_without_trend': 5, 'AR_1_with_trend': 6, 'AR_1_without_trend': 7, 'AR_2_with_trend': 8, 'AR_2_without_trend': 9, 'AR_3_with_trend': 10, 'AR_3_without_trend': 11, 'MA_1_with_trend': 12, 'MA_1_without_trend': 13, 'MA_2_with_trend': 14, 'MA_2_without_trend': 15, 'MA_3_with_trend': 16, 'MA_3_without_trend': 17}\n"
     ]
    }
   ],
   "source": [
    "# Generate a mapping for classes\n",
    "classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # {'AR_1_with_trend': 0, 'MA_1_without_trend': 1, ...}\n",
    "print(f\"Class to index mapping: {class_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Classes: ['ARMA_1_with_trend', 'ARMA_1_without_trend', 'ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend', 'ARMA_3_without_trend', 'AR_1_with_trend', 'AR_1_without_trend', 'AR_2_with_trend', 'AR_2_without_trend', 'AR_3_with_trend', 'AR_3_without_trend', 'MA_1_with_trend', 'MA_1_without_trend', 'MA_2_with_trend', 'MA_2_without_trend', 'MA_3_with_trend', 'MA_3_without_trend']\n",
      "Number of classes: 18\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Print class information\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Number of classes: {len(classes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset of 9 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (25200, 64, 64, 3), (25200,)\n",
      "Validation set: (7200, 64, 64, 3), (7200,)\n",
      "Test set: (3600, 64, 64, 3), (3600,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Subset of classes to start with (9 classes)\n",
    "subset_classes = classes[:9]\n",
    "subset_indices = [class_to_idx[cls] for cls in subset_classes]\n",
    "\n",
    "# Filter data and labels for the subset\n",
    "subset_mask = np.isin(labels, subset_indices)\n",
    "data_subset = data[subset_mask]\n",
    "labels_subset = labels[subset_mask]\n",
    "\n",
    "# Update labels for the subset (reindex for subset only)\n",
    "labels_subset = np.array([subset_indices.index(lbl) for lbl in labels_subset])\n",
    "\n",
    "# Step 1: Split into train (70%) and temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    data_subset, labels_subset, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split temp into validation (20%) and test (10%)\n",
    "# Temp is already 30%, so we divide it: 20% = (2/3 of temp), 10% = (1/3 of temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=1/3, random_state=42  # 1/3 of temp goes to test set\n",
    ")\n",
    "\n",
    "# Resize the datasets\n",
    "X_train_resized = tf.image.resize(X_train, [224, 224])\n",
    "X_val_resized = tf.image.resize(X_val, [224, 224])\n",
    "X_test_resized = tf.image.resize(X_test, [224, 224])\n",
    "\n",
    "\n",
    "# Print dataset sizes to verify the split\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Pretrained model: MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 419ms/step - accuracy: 0.2036 - loss: 2.2150 - val_accuracy: 0.2649 - val_loss: 1.7333\n",
      "Epoch 2/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 424ms/step - accuracy: 0.2457 - loss: 1.8097 - val_accuracy: 0.2983 - val_loss: 1.6714\n",
      "Epoch 3/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 416ms/step - accuracy: 0.2635 - loss: 1.7363 - val_accuracy: 0.2918 - val_loss: 1.6436\n",
      "Epoch 4/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 406ms/step - accuracy: 0.2674 - loss: 1.6959 - val_accuracy: 0.3093 - val_loss: 1.6147\n",
      "Epoch 5/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 412ms/step - accuracy: 0.2826 - loss: 1.6595 - val_accuracy: 0.3110 - val_loss: 1.5915\n",
      "Epoch 6/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 407ms/step - accuracy: 0.2919 - loss: 1.6297 - val_accuracy: 0.3236 - val_loss: 1.5681\n",
      "Epoch 7/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 410ms/step - accuracy: 0.2997 - loss: 1.6053 - val_accuracy: 0.3179 - val_loss: 1.5547\n",
      "Epoch 8/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 408ms/step - accuracy: 0.3113 - loss: 1.5775 - val_accuracy: 0.3257 - val_loss: 1.5374\n",
      "Epoch 9/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 410ms/step - accuracy: 0.3085 - loss: 1.5672 - val_accuracy: 0.3193 - val_loss: 1.5260\n",
      "Epoch 10/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 411ms/step - accuracy: 0.3090 - loss: 1.5440 - val_accuracy: 0.3274 - val_loss: 1.5073\n",
      "Epoch 11/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 420ms/step - accuracy: 0.3163 - loss: 1.5309 - val_accuracy: 0.3283 - val_loss: 1.4992\n",
      "Epoch 12/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 406ms/step - accuracy: 0.3195 - loss: 1.5150 - val_accuracy: 0.3271 - val_loss: 1.4906\n",
      "Epoch 13/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 406ms/step - accuracy: 0.3266 - loss: 1.5042 - val_accuracy: 0.3342 - val_loss: 1.4772\n",
      "Epoch 14/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 409ms/step - accuracy: 0.3240 - loss: 1.4983 - val_accuracy: 0.3332 - val_loss: 1.4670\n",
      "Epoch 15/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 406ms/step - accuracy: 0.3227 - loss: 1.4865 - val_accuracy: 0.3360 - val_loss: 1.4609\n",
      "Epoch 16/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 228ms/step - accuracy: 0.3287 - loss: 1.4725 - val_accuracy: 0.3221 - val_loss: 1.4645\n",
      "Epoch 17/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 254ms/step - accuracy: 0.3313 - loss: 1.4698 - val_accuracy: 0.3290 - val_loss: 1.4476\n",
      "Epoch 18/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 423ms/step - accuracy: 0.3345 - loss: 1.4579 - val_accuracy: 0.3382 - val_loss: 1.4406\n",
      "Epoch 19/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 418ms/step - accuracy: 0.3410 - loss: 1.4494 - val_accuracy: 0.3281 - val_loss: 1.4432\n",
      "Epoch 20/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 477ms/step - accuracy: 0.3354 - loss: 1.4475 - val_accuracy: 0.3324 - val_loss: 1.4369\n",
      "Epoch 21/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 637ms/step - accuracy: 0.3401 - loss: 1.4415 - val_accuracy: 0.3457 - val_loss: 1.4268\n",
      "Epoch 22/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 632ms/step - accuracy: 0.3465 - loss: 1.4286 - val_accuracy: 0.3444 - val_loss: 1.4249\n",
      "Epoch 23/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 630ms/step - accuracy: 0.3479 - loss: 1.4271 - val_accuracy: 0.3324 - val_loss: 1.4296\n",
      "Epoch 24/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 629ms/step - accuracy: 0.3476 - loss: 1.4282 - val_accuracy: 0.3457 - val_loss: 1.4138\n",
      "Epoch 25/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 639ms/step - accuracy: 0.3453 - loss: 1.4206 - val_accuracy: 0.3524 - val_loss: 1.4096\n",
      "Epoch 26/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 628ms/step - accuracy: 0.3448 - loss: 1.4147 - val_accuracy: 0.3431 - val_loss: 1.4107\n",
      "Epoch 27/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 637ms/step - accuracy: 0.3437 - loss: 1.4102 - val_accuracy: 0.3481 - val_loss: 1.4023\n",
      "Epoch 28/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 631ms/step - accuracy: 0.3592 - loss: 1.4052 - val_accuracy: 0.3365 - val_loss: 1.4098\n",
      "Epoch 29/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 640ms/step - accuracy: 0.3488 - loss: 1.4058 - val_accuracy: 0.3469 - val_loss: 1.4004\n",
      "Epoch 30/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 666ms/step - accuracy: 0.3533 - loss: 1.4043 - val_accuracy: 0.3558 - val_loss: 1.3974\n",
      "Epoch 31/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 639ms/step - accuracy: 0.3546 - loss: 1.3989 - val_accuracy: 0.3500 - val_loss: 1.3944\n",
      "Epoch 32/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 558ms/step - accuracy: 0.3630 - loss: 1.3920 - val_accuracy: 0.3493 - val_loss: 1.3936\n",
      "Epoch 33/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 560ms/step - accuracy: 0.3581 - loss: 1.3920 - val_accuracy: 0.3557 - val_loss: 1.3905\n",
      "Epoch 34/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 631ms/step - accuracy: 0.3580 - loss: 1.3883 - val_accuracy: 0.3589 - val_loss: 1.3842\n",
      "Epoch 35/35\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 633ms/step - accuracy: 0.3645 - loss: 1.3878 - val_accuracy: 0.3481 - val_loss: 1.3908\n",
      "113/113 - 59s - 523ms/step - accuracy: 0.3628 - loss: 1.3932\n",
      "Test Accuracy: 0.36\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the MobileNetV2-based model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    # Load the MobileNetV2 base model pre-trained on ImageNet\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model layers\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Pooling to reduce feature map dimensions\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(1e-4)),  # First dense layer\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(1e-4)),  # Second dense layer\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(1e-4)),  # Third dense layer\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(1e-4)),  # Fourth dense layer\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')  # Final output layer\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (224, 224, 3)  # Input shape for MobileNetV2\n",
    "num_classes = len(subset_classes)  # Number of classes in your subset (9)\n",
    "\n",
    "# Resize the datasets to 224x224\n",
    "X_train_resized = tf.image.resize(X_train, [224, 224])\n",
    "X_val_resized = tf.image.resize(X_val, [224, 224])\n",
    "X_test_resized = tf.image.resize(X_test, [224, 224])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_mobilenetv2(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Learning rate for initial training\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train_categorical,  # Resized and normalized training data\n",
    "    validation_data=(X_val_resized, y_val_categorical),  # Resized and normalized validation data\n",
    "    epochs=35,  # Initial training epochs\n",
    "    batch_size=32,  # Batch size\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test_categorical, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 817ms/step - accuracy: 0.2842 - loss: 2.0363 - val_accuracy: 0.3313 - val_loss: 1.4357\n",
      "Epoch 2/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m643s\u001b[0m 816ms/step - accuracy: 0.3347 - loss: 1.4303 - val_accuracy: 0.3314 - val_loss: 1.4307\n",
      "Epoch 3/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 814ms/step - accuracy: 0.3478 - loss: 1.4058 - val_accuracy: 0.3496 - val_loss: 1.4091\n",
      "Epoch 4/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 801ms/step - accuracy: 0.3624 - loss: 1.3808 - val_accuracy: 0.3111 - val_loss: 1.4805\n",
      "Epoch 5/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 801ms/step - accuracy: 0.3697 - loss: 1.3586 - val_accuracy: 0.3544 - val_loss: 1.3878\n",
      "Epoch 6/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 561ms/step - accuracy: 0.3780 - loss: 1.3464 - val_accuracy: 0.3549 - val_loss: 1.3871\n",
      "Epoch 7/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 519ms/step - accuracy: 0.3893 - loss: 1.3250 - val_accuracy: 0.3597 - val_loss: 1.3778\n",
      "Epoch 8/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 481ms/step - accuracy: 0.3934 - loss: 1.3168 - val_accuracy: 0.3529 - val_loss: 1.3998\n",
      "Epoch 9/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 278ms/step - accuracy: 0.4059 - loss: 1.2919 - val_accuracy: 0.3679 - val_loss: 1.3851\n",
      "Epoch 10/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 277ms/step - accuracy: 0.4106 - loss: 1.2706 - val_accuracy: 0.3633 - val_loss: 1.3815\n",
      "Epoch 11/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 279ms/step - accuracy: 0.4175 - loss: 1.2650 - val_accuracy: 0.3557 - val_loss: 1.4037\n",
      "Epoch 12/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 279ms/step - accuracy: 0.4306 - loss: 1.2458 - val_accuracy: 0.3607 - val_loss: 1.4448\n",
      "Epoch 13/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 279ms/step - accuracy: 0.4360 - loss: 1.2250 - val_accuracy: 0.3603 - val_loss: 1.4291\n",
      "Epoch 14/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 278ms/step - accuracy: 0.4425 - loss: 1.2110 - val_accuracy: 0.3496 - val_loss: 1.5114\n",
      "Epoch 15/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 278ms/step - accuracy: 0.4485 - loss: 1.1889 - val_accuracy: 0.3672 - val_loss: 1.4649\n",
      "Epoch 16/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 278ms/step - accuracy: 0.4531 - loss: 1.1789 - val_accuracy: 0.3583 - val_loss: 1.5031\n",
      "Epoch 17/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 278ms/step - accuracy: 0.4609 - loss: 1.1690 - val_accuracy: 0.3593 - val_loss: 1.4935\n",
      "Epoch 18/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 279ms/step - accuracy: 0.4633 - loss: 1.1546 - val_accuracy: 0.3672 - val_loss: 1.4976\n",
      "Epoch 19/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 278ms/step - accuracy: 0.4736 - loss: 1.1344 - val_accuracy: 0.3615 - val_loss: 1.5511\n",
      "Epoch 20/20\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 278ms/step - accuracy: 0.4820 - loss: 1.1218 - val_accuracy: 0.3676 - val_loss: 1.5481\n"
     ]
    }
   ],
   "source": [
    "base_model = model.layers[0]  # Access the MobileNetV2 base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Optionally freeze earlier layers for stability\n",
    "for layer in base_model.layers[:120]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine_tune = model.fit(\n",
    "    X_train_resized, y_train_categorical,\n",
    "    validation_data=(X_val_resized, y_val_categorical),\n",
    "    epochs=20,  # Fine-tuning epochs\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- MobileNet with different blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_12     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_12     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,311,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,161,353</span> (15.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,161,353\u001b[0m (15.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,903,369</span> (7.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,903,369\u001b[0m (7.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 504ms/step - accuracy: 0.2430 - loss: 1.9137 - val_accuracy: 0.3075 - val_loss: 1.6463 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 511ms/step - accuracy: 0.2794 - loss: 1.7131 - val_accuracy: 0.3111 - val_loss: 1.6201 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 533ms/step - accuracy: 0.2865 - loss: 1.6704 - val_accuracy: 0.3153 - val_loss: 1.5907 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 516ms/step - accuracy: 0.3037 - loss: 1.6341 - val_accuracy: 0.3085 - val_loss: 1.6006 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 513ms/step - accuracy: 0.3047 - loss: 1.6171 - val_accuracy: 0.3239 - val_loss: 1.5592 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 526ms/step - accuracy: 0.3081 - loss: 1.6028 - val_accuracy: 0.3254 - val_loss: 1.5579 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 516ms/step - accuracy: 0.3105 - loss: 1.5869 - val_accuracy: 0.3322 - val_loss: 1.5358 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 495ms/step - accuracy: 0.3207 - loss: 1.5714 - val_accuracy: 0.3286 - val_loss: 1.5340 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 428ms/step - accuracy: 0.3295 - loss: 1.5539 - val_accuracy: 0.3319 - val_loss: 1.5277 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 419ms/step - accuracy: 0.3246 - loss: 1.5440 - val_accuracy: 0.3326 - val_loss: 1.5249 - learning_rate: 1.0000e-04\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 314ms/step - accuracy: 0.3426 - loss: 1.5167\n",
      "Validation Accuracy: 33.26%\n",
      "Epoch 1/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 855ms/step - accuracy: 0.2605 - loss: 2.0807 - val_accuracy: 0.2387 - val_loss: 1.7301 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 836ms/step - accuracy: 0.3213 - loss: 1.5461 - val_accuracy: 0.2599 - val_loss: 1.6919 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 899ms/step - accuracy: 0.3543 - loss: 1.4982 - val_accuracy: 0.3139 - val_loss: 1.5569 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 875ms/step - accuracy: 0.3711 - loss: 1.4583 - val_accuracy: 0.3336 - val_loss: 1.5304 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 852ms/step - accuracy: 0.3886 - loss: 1.4262 - val_accuracy: 0.3501 - val_loss: 1.4868 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 870ms/step - accuracy: 0.3987 - loss: 1.4053 - val_accuracy: 0.3621 - val_loss: 1.4698 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 829ms/step - accuracy: 0.4153 - loss: 1.3642 - val_accuracy: 0.3688 - val_loss: 1.4732 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 865ms/step - accuracy: 0.4271 - loss: 1.3376 - val_accuracy: 0.3615 - val_loss: 1.4978 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 855ms/step - accuracy: 0.4500 - loss: 1.2986 - val_accuracy: 0.3649 - val_loss: 1.5133 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 850ms/step - accuracy: 0.4642 - loss: 1.2595 - val_accuracy: 0.3613 - val_loss: 1.5541 - learning_rate: 1.0000e-05\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 324ms/step - accuracy: 0.3741 - loss: 1.4701\n",
      "Test Accuracy: 37.08%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define the transfer learning model\n",
    "def build_transfer_model(input_shape, num_classes):\n",
    "    # Load the MobileNetV2 model pre-trained on ImageNet\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model layers to prevent training\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom classification head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Reduce spatial dimensions\n",
    "        \n",
    "        # Additional convolutional blocks\n",
    "        Dense(1024, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "        Dropout(0.6),\n",
    "        Dense(512, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_shape = (224, 224, 3)  # RGB images resized to 224x224\n",
    "num_classes = len(subset_classes)\n",
    "batch_size = 32\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_transfer_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val_resized, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Fine-tune the base model\n",
    "base_model = model.layers[0]  # Access the base model (MobileNetV2)\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first few layers for stability\n",
    "for layer in base_model.layers[:50]:  # Adjust based on experimentation\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_10     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_10     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,079,241</span> (11.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,079,241\u001b[0m (11.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">821,257</span> (3.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m821,257\u001b[0m (3.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 235ms/step - accuracy: 0.2263 - loss: 1.9274 - val_accuracy: 0.2926 - val_loss: 1.5985 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 230ms/step - accuracy: 0.2693 - loss: 1.6707 - val_accuracy: 0.2925 - val_loss: 1.5659 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 241ms/step - accuracy: 0.2790 - loss: 1.6310 - val_accuracy: 0.3124 - val_loss: 1.5436 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 499ms/step - accuracy: 0.2982 - loss: 1.5862 - val_accuracy: 0.3129 - val_loss: 1.5263 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 419ms/step - accuracy: 0.2992 - loss: 1.5738 - val_accuracy: 0.3199 - val_loss: 1.5096 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 225ms/step - accuracy: 0.2985 - loss: 1.5628 - val_accuracy: 0.3293 - val_loss: 1.5064 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 230ms/step - accuracy: 0.3106 - loss: 1.5448 - val_accuracy: 0.3336 - val_loss: 1.4928 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 239ms/step - accuracy: 0.3134 - loss: 1.5317 - val_accuracy: 0.3378 - val_loss: 1.4843 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 320ms/step - accuracy: 0.3183 - loss: 1.5175 - val_accuracy: 0.3294 - val_loss: 1.4805 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 380ms/step - accuracy: 0.3157 - loss: 1.5123 - val_accuracy: 0.3396 - val_loss: 1.4724 - learning_rate: 1.0000e-04\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 312ms/step - accuracy: 0.3426 - loss: 1.4663\n",
      "Validation Accuracy: 33.96%\n",
      "Epoch 1/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 761ms/step - accuracy: 0.2484 - loss: 2.2324 - val_accuracy: 0.2569 - val_loss: 1.6432 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 795ms/step - accuracy: 0.3118 - loss: 1.5198 - val_accuracy: 0.2735 - val_loss: 1.6071 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 833ms/step - accuracy: 0.3396 - loss: 1.4749 - val_accuracy: 0.3181 - val_loss: 1.5115 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 833ms/step - accuracy: 0.3597 - loss: 1.4400 - val_accuracy: 0.3454 - val_loss: 1.4608 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 835ms/step - accuracy: 0.3693 - loss: 1.4091 - val_accuracy: 0.3613 - val_loss: 1.4358 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 834ms/step - accuracy: 0.3918 - loss: 1.3864 - val_accuracy: 0.3650 - val_loss: 1.4261 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 837ms/step - accuracy: 0.3961 - loss: 1.3599 - val_accuracy: 0.3549 - val_loss: 1.4360 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 698ms/step - accuracy: 0.4133 - loss: 1.3302 - val_accuracy: 0.3625 - val_loss: 1.4501 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 485ms/step - accuracy: 0.4267 - loss: 1.2964 - val_accuracy: 0.3697 - val_loss: 1.4553 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 485ms/step - accuracy: 0.4388 - loss: 1.2622 - val_accuracy: 0.3592 - val_loss: 1.4757 - learning_rate: 1.0000e-05\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 172ms/step - accuracy: 0.3749 - loss: 1.4289\n",
      "Test Accuracy: 36.61%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define the transfer learning model\n",
    "def build_transfer_model(input_shape, num_classes):\n",
    "    # Load the MobileNetV2 model pre-trained on ImageNet\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model layers to prevent training\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom classification head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "        Dropout(0.6),\n",
    "        Dense(256, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "input_shape = (224, 224, 3)  # RGB images resized to 224x224\n",
    "num_classes = len(subset_classes)\n",
    "batch_size = 32\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_transfer_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val_resized, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Fine-tune the base model\n",
    "base_model = model.layers[0]  # Access the base model (MobileNetV2)\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first few layers for stability\n",
    "for layer in base_model.layers[:50]:  # Adjust based on experimentation\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---Subset of 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess only 10 classes for training\n",
    "def filter_classes(data, labels, classes, subset_size=10):\n",
    "    \"\"\"Filters data and labels to only include a subset of classes.\"\"\"\n",
    "    selected_classes = classes[:subset_size]\n",
    "    class_indices = [classes.index(cls) for cls in selected_classes]\n",
    "    filtered_data = []\n",
    "    filtered_labels = []\n",
    "    for img, lbl in zip(data, labels):\n",
    "        if lbl in class_indices:\n",
    "            filtered_data.append(img)\n",
    "            filtered_labels.append(lbl)\n",
    "    return np.array(filtered_data), np.array(filtered_labels), selected_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your existing code to load the dataset\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Filter the first 10 classes for training\n",
    "subset_size = 10\n",
    "filtered_data, filtered_labels, selected_classes = filter_classes(data, labels, classes, subset_size=subset_size)\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "filtered_data, filtered_labels = shuffle(filtered_data, filtered_labels, random_state=123)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(filtered_data, filtered_labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert numerical labels to one-hot encoding\n",
    "if len(y_train.shape) == 1:\n",
    "    y_train = to_categorical(y_train, num_classes=len(selected_classes))\n",
    "    y_val = to_categorical(y_val, num_classes=len(selected_classes))\n",
    "    y_test = to_categorical(y_test, num_classes=len(selected_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 182ms/step - accuracy: 0.1558 - loss: 14.8425 - val_accuracy: 0.1972 - val_loss: 4.2489 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.1994 - loss: 3.6785 - val_accuracy: 0.2033 - val_loss: 2.6674 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2034 - loss: 2.5492 - val_accuracy: 0.2069 - val_loss: 2.1977 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 177ms/step - accuracy: 0.1977 - loss: 2.1480 - val_accuracy: 0.2035 - val_loss: 1.9631 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2044 - loss: 1.9366 - val_accuracy: 0.2085 - val_loss: 1.8336 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2061 - loss: 1.8244 - val_accuracy: 0.2140 - val_loss: 1.7650 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.2000 - loss: 1.7639 - val_accuracy: 0.2085 - val_loss: 1.7281 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 174ms/step - accuracy: 0.2002 - loss: 1.7326 - val_accuracy: 0.2052 - val_loss: 1.7071 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2025 - loss: 1.7114 - val_accuracy: 0.2145 - val_loss: 1.6958 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 177ms/step - accuracy: 0.2087 - loss: 1.6975 - val_accuracy: 0.2045 - val_loss: 1.6866 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 177ms/step - accuracy: 0.2038 - loss: 1.6936 - val_accuracy: 0.1982 - val_loss: 1.6890 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 177ms/step - accuracy: 0.2012 - loss: 1.6880 - val_accuracy: 0.2114 - val_loss: 1.6772 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.2030 - loss: 1.6824 - val_accuracy: 0.2116 - val_loss: 1.6735 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 174ms/step - accuracy: 0.2044 - loss: 1.6788 - val_accuracy: 0.2104 - val_loss: 1.6712 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2024 - loss: 1.6761 - val_accuracy: 0.2060 - val_loss: 1.6684 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.2071 - loss: 1.6718 - val_accuracy: 0.2099 - val_loss: 1.6654 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 182ms/step - accuracy: 0.2069 - loss: 1.6694 - val_accuracy: 0.2107 - val_loss: 1.6630 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 188ms/step - accuracy: 0.2024 - loss: 1.6689 - val_accuracy: 0.2094 - val_loss: 1.6626 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 186ms/step - accuracy: 0.2054 - loss: 1.6665 - val_accuracy: 0.2126 - val_loss: 1.6599 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 187ms/step - accuracy: 0.2085 - loss: 1.6644 - val_accuracy: 0.2039 - val_loss: 1.6592 - learning_rate: 1.0000e-04\n",
      "Epoch 1/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 439ms/step - accuracy: 0.2114 - loss: 1.6711 - val_accuracy: 0.2066 - val_loss: 1.6596 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 437ms/step - accuracy: 0.2340 - loss: 1.6126 - val_accuracy: 0.2064 - val_loss: 1.6514 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 430ms/step - accuracy: 0.2494 - loss: 1.5948 - val_accuracy: 0.2066 - val_loss: 1.6747 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 431ms/step - accuracy: 0.2532 - loss: 1.5869 - val_accuracy: 0.2447 - val_loss: 1.6039 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 421ms/step - accuracy: 0.2564 - loss: 1.5789 - val_accuracy: 0.1996 - val_loss: 2.4073 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 425ms/step - accuracy: 0.2618 - loss: 1.5747 - val_accuracy: 0.1994 - val_loss: 2.1046 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.2590 - loss: 1.5710\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 425ms/step - accuracy: 0.2590 - loss: 1.5710 - val_accuracy: 0.2229 - val_loss: 1.6322 - learning_rate: 1.0000e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 427ms/step - accuracy: 0.2651 - loss: 1.5661 - val_accuracy: 0.2670 - val_loss: 1.5664 - learning_rate: 5.0000e-06\n",
      "Epoch 9/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2715 - loss: 1.5578 - val_accuracy: 0.2256 - val_loss: 1.6825 - learning_rate: 5.0000e-06\n",
      "Epoch 10/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2683 - loss: 1.5595 - val_accuracy: 0.2676 - val_loss: 1.5833 - learning_rate: 5.0000e-06\n",
      "Epoch 11/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 0.2742 - loss: 1.5531\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2742 - loss: 1.5531 - val_accuracy: 0.2290 - val_loss: 1.7029 - learning_rate: 5.0000e-06\n",
      "Epoch 12/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2732 - loss: 1.5497 - val_accuracy: 0.2495 - val_loss: 1.5957 - learning_rate: 2.5000e-06\n",
      "Epoch 13/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 423ms/step - accuracy: 0.2863 - loss: 1.5417 - val_accuracy: 0.2766 - val_loss: 1.5700 - learning_rate: 2.5000e-06\n",
      "125/125 - 10s - 81ms/step - accuracy: 0.2862 - loss: 1.5626\n",
      "Test Accuracy: 0.29\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Number of classes after filtering\n",
    "num_classes = len(selected_classes)  # Should be 10\n",
    "\n",
    "# Define the ResNet50 model\n",
    "def build_resnet50(input_shape, num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Reduces feature map dimensions\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),  # Regularization\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer dynamically set to 10 classes\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (64, 64, 3)\n",
    "model = build_resnet50(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for learning rate adjustment and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune the base model\n",
    "base_model = model.layers[0]  # Extract base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Unfreeze the last 50 layers for fine-tuning\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Applying Hilberts Space Filling Curve on AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "\n",
    "def apply_hilbert_curve(image, n=2):\n",
    "    \"\"\"\n",
    "    Applies the Hilbert curve to reorder image pixels.\n",
    "    Args:\n",
    "        image: Input 2D grayscale or 3D RGB image (numpy array).\n",
    "        n: Number of dimensions for the Hilbert curve (default is 2D).\n",
    "    Returns:\n",
    "        Transformed image reordered using the Hilbert curve.\n",
    "    \"\"\"\n",
    "    if image.shape[0] != image.shape[1]:\n",
    "        raise ValueError(\"Image must be square for Hilbert curve transformation.\")\n",
    "\n",
    "    size = image.shape[0]  # Assuming square images\n",
    "    p = int(np.log2(size))  # Determine the order of the Hilbert curve\n",
    "\n",
    "    if 2**p != size:\n",
    "        raise ValueError(f\"Image size must be a power of 2. Current size: {size}\")\n",
    "\n",
    "    hilbert_curve = HilbertCurve(p, n)\n",
    "    coords = [hilbert_curve.point_from_distance(d) for d in range(2**(p * n))]\n",
    "\n",
    "    # Reorder image pixels based on Hilbert curve\n",
    "    flat_image = image.flatten()\n",
    "    reordered_image = np.zeros_like(flat_image)\n",
    "    for i, (x, y) in enumerate(coords):\n",
    "        reordered_image[i] = flat_image[x * size + y]\n",
    "\n",
    "    return reordered_image.reshape(image.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess only 8 classes for training\n",
    "def filter_classes(data, labels, classes, subset_size=8):\n",
    "    \"\"\"Filters data and labels to only include a subset of classes.\"\"\"\n",
    "    selected_classes = classes[:subset_size]\n",
    "    class_indices = [classes.index(cls) for cls in selected_classes]\n",
    "    filtered_data = []\n",
    "    filtered_labels = []\n",
    "    for img, lbl in zip(data, labels):\n",
    "        if lbl in class_indices:\n",
    "            filtered_data.append(img)\n",
    "            filtered_labels.append(lbl)\n",
    "    return np.array(filtered_data), np.array(filtered_labels), selected_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a subset of 8 classes (incremental training) with hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your existing code to load the dataset\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Filter the first 8 classes for training\n",
    "subset_size = 8\n",
    "filtered_data, filtered_labels, selected_classes = filter_classes(data, labels, classes, subset_size=subset_size)\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "filtered_data, filtered_labels = shuffle(filtered_data, filtered_labels, random_state=123)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(filtered_data, filtered_labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert numerical labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(selected_classes))\n",
    "y_val = to_categorical(y_val, num_classes=len(selected_classes))\n",
    "y_test = to_categorical(y_test, num_classes=len(selected_classes))\n",
    "\n",
    "\n",
    "# Apply Hilbert Curve to data\n",
    "X_train = np.array([apply_hilbert_curve(img) for img in X_train])\n",
    "X_val = np.array([apply_hilbert_curve(img) for img in X_val])\n",
    "X_test = np.array([apply_hilbert_curve(img) for img in X_test])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish  \n",
    "\n",
    "def build_enhanced_alexnet(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # First Conv Block\n",
    "        Conv2D(96, (11, 11), strides=4, input_shape=input_shape, kernel_regularizer=l2(0.02)),\n",
    "        BatchNormalization(),\n",
    "        # Swish activation applied\n",
    "        AveragePooling2D((2, 2), strides=2),\n",
    "\n",
    "        # Second Conv Block\n",
    "        Conv2D(256, (5, 5), padding='same', kernel_regularizer=l2(0.02)),\n",
    "        BatchNormalization(),\n",
    "        AveragePooling2D((2, 2), strides=2),\n",
    "\n",
    "        # Third Conv Block\n",
    "        Conv2D(384, (3, 3), padding='same', kernel_regularizer=l2(0.02)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Fourth Conv Block\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        AveragePooling2D((2, 2), strides=2),\n",
    "\n",
    "        # Additional Conv Layer for feature enrichment\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling2D(),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        Dense(4096, kernel_regularizer=l2(0.02), activation=swish),  # Swish activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(2048, kernel_regularizer=l2(0.02), activation=swish),  # Swish activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 78ms/step - accuracy: 0.2641 - loss: 60.1118 - val_accuracy: 0.1253 - val_loss: 36.3542\n",
      "Epoch 2/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.2822 - loss: 24.8802 - val_accuracy: 0.1297 - val_loss: 24.5882\n",
      "Epoch 3/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2755 - loss: 9.7143 - val_accuracy: 0.1372 - val_loss: 19.3844\n",
      "Epoch 4/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2875 - loss: 4.6135 - val_accuracy: 0.2442 - val_loss: 7.6495\n",
      "Epoch 5/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 80ms/step - accuracy: 0.2893 - loss: 3.0477 - val_accuracy: 0.1214 - val_loss: 27.4130\n",
      "Epoch 6/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2824 - loss: 2.5058 - val_accuracy: 0.2125 - val_loss: 3.3555\n",
      "Epoch 7/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2854 - loss: 2.2686 - val_accuracy: 0.1203 - val_loss: 25.8767\n",
      "Epoch 8/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2914 - loss: 2.1769 - val_accuracy: 0.1214 - val_loss: 15.5666\n",
      "Epoch 9/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.2867 - loss: 2.0852 - val_accuracy: 0.1255 - val_loss: 34.1144\n",
      "Epoch 10/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2949 - loss: 2.0309 - val_accuracy: 0.1203 - val_loss: 52.9481\n",
      "Epoch 11/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2868 - loss: 1.9710 - val_accuracy: 0.2134 - val_loss: 4.7536\n",
      "Epoch 12/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.2949 - loss: 1.9136 - val_accuracy: 0.1298 - val_loss: 72.0177\n",
      "Epoch 13/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2981 - loss: 1.8661 - val_accuracy: 0.1327 - val_loss: 31.6529\n",
      "Epoch 14/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 80ms/step - accuracy: 0.2957 - loss: 1.8325 - val_accuracy: 0.1214 - val_loss: 21.5069\n",
      "Epoch 15/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 81ms/step - accuracy: 0.2920 - loss: 1.8174 - val_accuracy: 0.1214 - val_loss: 8.3147\n",
      "100/100 - 1s - 14ms/step - accuracy: 0.1209 - loss: 8.5354\n",
      "Test Accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "model = build_enhanced_alexnet(input_shape, len(selected_classes))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Pretrained model AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# AlexNet-inspired architecture with modifications\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(128, (11, 11), strides=4, activation=swish, input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D((3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))  # Regularization with dropout\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(256, (5, 5), activation=swish, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D((3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(384, (3, 3), activation=swish, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(384, (3, 3), activation=swish, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(512, (3, 3), activation=swish, padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D((3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(4096, activation=swish, kernel_regularizer=l2(0.01)))  # Added L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout to reduce overfitting\n",
    "\n",
    "    # 2nd Dense Layer\n",
    "    model.add(Dense(4096, activation=swish, kernel_regularizer=l2(0.01)))  # Added L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # 3rd Dense Layer\n",
    "    model.add(Dense(2048, activation=swish))  # No L2 regularization here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 167ms/step - accuracy: 0.2056 - loss: 53.1360 - val_accuracy: 0.1472 - val_loss: 42.9628 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 166ms/step - accuracy: 0.2440 - loss: 35.1721 - val_accuracy: 0.2603 - val_loss: 23.2538 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 165ms/step - accuracy: 0.2506 - loss: 20.7150 - val_accuracy: 0.2620 - val_loss: 12.6875 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 163ms/step - accuracy: 0.2474 - loss: 11.3557 - val_accuracy: 0.2338 - val_loss: 7.3159 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 159ms/step - accuracy: 0.2554 - loss: 6.2414 - val_accuracy: 0.2427 - val_loss: 4.1052 - learning_rate: 1.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 162ms/step - accuracy: 0.2517 - loss: 3.7598 - val_accuracy: 0.1117 - val_loss: 4.4493 - learning_rate: 1.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 159ms/step - accuracy: 0.2408 - loss: 2.6986 - val_accuracy: 0.2414 - val_loss: 2.1916 - learning_rate: 1.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 163ms/step - accuracy: 0.2527 - loss: 2.3096 - val_accuracy: 0.1295 - val_loss: 24.9149 - learning_rate: 1.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 158ms/step - accuracy: 0.2557 - loss: 2.1288 - val_accuracy: 0.2336 - val_loss: 3.5971 - learning_rate: 1.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 162ms/step - accuracy: 0.2505 - loss: 2.1020 - val_accuracy: 0.2617 - val_loss: 2.0676 - learning_rate: 1.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 161ms/step - accuracy: 0.2576 - loss: 2.1052 - val_accuracy: 0.2236 - val_loss: 1.8821 - learning_rate: 1.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 160ms/step - accuracy: 0.2464 - loss: 2.0443 - val_accuracy: 0.2442 - val_loss: 3.3729 - learning_rate: 1.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 158ms/step - accuracy: 0.2524 - loss: 2.0164 - val_accuracy: 0.2594 - val_loss: 2.0781 - learning_rate: 1.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 159ms/step - accuracy: 0.2556 - loss: 2.0170 - val_accuracy: 0.1538 - val_loss: 2.4026 - learning_rate: 1.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 158ms/step - accuracy: 0.2540 - loss: 1.9217 - val_accuracy: 0.1564 - val_loss: 2.3400 - learning_rate: 5.0000e-05\n",
      "100/100 - 3s - 27ms/step - accuracy: 0.1475 - loss: 2.3549\n",
      "Test Accuracy: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "model = build_modified_alexnet(input_shape, len(selected_classes))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- CNN with data augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_subset shape: (28001, 64, 64, 3)\n",
      "y_subset shape: (28001, 18)\n",
      "Epoch 1/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 38ms/step - accuracy: 0.2010 - loss: 11.1200 - val_accuracy: 0.0517 - val_loss: 57.2757 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 36ms/step - accuracy: 0.1960 - loss: 3.5387 - val_accuracy: 0.0515 - val_loss: 25.5853 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 37ms/step - accuracy: 0.1982 - loss: 3.3642 - val_accuracy: 0.0584 - val_loss: 10.6769 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 39ms/step - accuracy: 0.2038 - loss: 3.2414 - val_accuracy: 0.0491 - val_loss: 7.0207 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.2135 - loss: 3.1845 - val_accuracy: 0.1147 - val_loss: 6.7574 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.2110 - loss: 3.0236 - val_accuracy: 0.1124 - val_loss: 6.5164 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.2135 - loss: 2.8961 - val_accuracy: 0.0859 - val_loss: 7.8053 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 37ms/step - accuracy: 0.2157 - loss: 2.8697 - val_accuracy: 0.0960 - val_loss: 7.7451 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 36ms/step - accuracy: 0.2198 - loss: 2.8994 - val_accuracy: 0.0772 - val_loss: 8.1449 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 37ms/step - accuracy: 0.2233 - loss: 2.3505 - val_accuracy: 0.1174 - val_loss: 7.4649 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 37ms/step - accuracy: 0.2225 - loss: 2.2427 - val_accuracy: 0.1239 - val_loss: 7.8545 - learning_rate: 5.0000e-04\n",
      "Test Accuracy: 0.11\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(96, (11, 11), strides=4, input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))  # Added dropout to reduce overfitting\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(SeparableConv2D(256, (5, 5), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))  # Added dropout\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(SeparableConv2D(384, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.3))  # Added dropout\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(SeparableConv2D(384, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.3))  # Added dropout\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(SeparableConv2D(256, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Added dropout\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, kernel_regularizer=l2(0.01)))  # L2 regularization only here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2048, kernel_regularizer=l2(0.01)))  # L2 regularization only here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 3)  # Assuming the resized images are 64x64 with 3 color channels\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Build the model\n",
    "model = build_modified_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Subset selection logic\n",
    "subset_class_indices = [i for i in range(10)]  # Classes 0, 1, 2 (numerical indices)\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset indices are valid\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug: Print subset shapes\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n",
    "\n",
    "# Callbacks for Early Stopping and Learning Rate Reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    datagen.flow(X_subset, y_subset, batch_size=32),\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- CNN model with Hilbert Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Applying Hilbert Curve Transformation...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1114 - loss: 2.7660 - val_accuracy: 0.1310 - val_loss: 2.2660 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1271 - loss: 2.3229 - val_accuracy: 0.1401 - val_loss: 2.1977 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1350 - loss: 2.2474 - val_accuracy: 0.1435 - val_loss: 2.1802 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1382 - loss: 2.2097 - val_accuracy: 0.1478 - val_loss: 2.1581 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1449 - loss: 2.1850 - val_accuracy: 0.1460 - val_loss: 2.1575 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1507 - loss: 2.1670 - val_accuracy: 0.1452 - val_loss: 2.1569 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1575 - loss: 2.1517 - val_accuracy: 0.1407 - val_loss: 2.1757 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1574 - loss: 2.1427 - val_accuracy: 0.1477 - val_loss: 2.1534 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1659 - loss: 2.1280 - val_accuracy: 0.1424 - val_loss: 2.1545 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1730 - loss: 2.1229 - val_accuracy: 0.1579 - val_loss: 2.1441 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1749 - loss: 2.1105 - val_accuracy: 0.1503 - val_loss: 2.1476 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1893 - loss: 2.0954 - val_accuracy: 0.1350 - val_loss: 2.1892 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1915 - loss: 2.0884 - val_accuracy: 0.1475 - val_loss: 2.1730 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1982 - loss: 2.0785 - val_accuracy: 0.1358 - val_loss: 2.2101 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2023 - loss: 2.0624 - val_accuracy: 0.1488 - val_loss: 2.1599 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2118 - loss: 2.0435 - val_accuracy: 0.1544 - val_loss: 2.1677 - learning_rate: 5.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.2181 - loss: 2.0293 - val_accuracy: 0.1551 - val_loss: 2.1674 - learning_rate: 5.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2276 - loss: 2.0142 - val_accuracy: 0.1551 - val_loss: 2.1726 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2360 - loss: 1.9969 - val_accuracy: 0.1581 - val_loss: 2.1824 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2375 - loss: 1.9875 - val_accuracy: 0.1499 - val_loss: 2.1953 - learning_rate: 5.0000e-05\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.1582 - loss: 2.1385\n",
      "Test Accuracy: 15.82%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "image_size = (64, 64)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "hilbert_order = 5\n",
    "grid_size = 2 ** hilbert_order\n",
    "\n",
    "# Directory containing the dataset\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")\n",
    "\n",
    "# 1. Transform Images with Hilbert Curve\n",
    "def hilbert_transform(image, hilbert_order):\n",
    "    curve = HilbertCurve(hilbert_order, 2)\n",
    "    grid_size = 2 ** hilbert_order\n",
    "    indices = [curve.point_from_distance(i) for i in range(grid_size ** 2)]\n",
    "    transformed_image = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    flat_image = np.array(image).flatten()\n",
    "    for idx, value in zip(indices, flat_image):\n",
    "        transformed_image[idx[0], idx[1]] = value\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "def transform_dataset_with_hilbert(data, hilbert_order):\n",
    "    print(\"Applying Hilbert Curve Transformation...\")\n",
    "    transformed_data = []\n",
    "    for img in data:\n",
    "        # Ensure data is in uint8 and grayscale\n",
    "        if img.dtype != 'uint8':\n",
    "            img = (img * 255).astype('uint8')  # Scale to [0, 255] and convert to uint8\n",
    "        img_gray = Image.fromarray(img).convert('L')  # Convert to grayscale\n",
    "        hilbert_image = hilbert_transform(img_gray, hilbert_order)\n",
    "        transformed_data.append(hilbert_image)\n",
    "    return np.array(transformed_data)\n",
    "\n",
    "# 2. Load Data\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = Image.open(img_path).convert('RGB').resize(image_size)\n",
    "                data.append(np.array(img))\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "data = data / 255.0  # Normalize pixel values\n",
    "data, labels = shuffle(data, labels, random_state=123)\n",
    "\n",
    "# 3. Apply Hilbert Transformation\n",
    "data = (data * 255).astype('uint8')  # Scale to [0, 255] and convert to uint8\n",
    "hilbert_data = transform_dataset_with_hilbert(data, hilbert_order)\n",
    "\n",
    "# Reshape Hilbert-transformed data for CNN input\n",
    "hilbert_data = hilbert_data.reshape(-1, grid_size, grid_size, 1)\n",
    "\n",
    "# 4. Split Data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(hilbert_data, labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(classes))\n",
    "y_val = to_categorical(y_val, num_classes=len(classes))\n",
    "y_test = to_categorical(y_test, num_classes=len(classes))\n",
    "\n",
    "# 5. CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes=len(classes))\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    #tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (72000, 64, 64, 3)\n",
      "Data dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Data dtype:\", data.dtype)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
