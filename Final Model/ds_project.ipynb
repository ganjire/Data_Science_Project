{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating artificial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script generates and saves synthetic time series data using AR, MA, and ARMA models with optional trend and seasonality. It creates 4000 time series per class, stores them in a structured directory, and saves them as images. The script also verifies the number of images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)  # For reproducibility\n",
    "n_series = 4000   # Number of time series per class\n",
    "n_points = 500    # Number of data points in each time series\n",
    "\n",
    "\n",
    "\n",
    "output_dir = os.path.expanduser(\"~/timeseries_data\")  # Creates the directory in your home folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AR, MA and ARMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate AR, MA, or ARMA data with optional trend and seasonality\n",
    "def generate_time_series(model_type, order, n_points, trend_strength=0.1, seasonality_amplitude=0.5, seasonality_period=50, include_trend=True):\n",
    "   if model_type == 'AR':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -params]\n",
    "       ma = np.array([1])\n",
    "   elif model_type == 'MA':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.array([1])\n",
    "       ma = np.r_[1, params]\n",
    "   elif model_type == 'ARMA':\n",
    "       ar_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ma_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -ar_params]\n",
    "       ma = np.r_[1, ma_params]\n",
    "   else:\n",
    "       raise ValueError(\"Invalid model type. Use 'AR', 'MA', or 'ARMA'.\")\n",
    "   \n",
    "   \n",
    "      # Generate the process\n",
    "   process = sm.tsa.ArmaProcess(ar, ma)\n",
    "   data = process.generate_sample(nsample=n_points)\n",
    "   if include_trend:\n",
    "       trend = np.linspace(0, trend_strength * n_points, n_points)\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += trend + seasonality\n",
    "   else:\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += seasonality\n",
    "   return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models for orders 1-3 with and without trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loop to generate and save time series plots for each class\n",
    "model_types = ['AR', 'MA', 'ARMA']\n",
    "orders = [1, 2, 3]\n",
    "for model_type in model_types:\n",
    "   for order in orders:\n",
    "       for include_trend in [True, False]:\n",
    "           class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "           class_dir = os.path.join(output_dir, class_label)\n",
    "           os.makedirs(class_dir, exist_ok=True)\n",
    "           for i in range(n_series):\n",
    "               data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "               \n",
    "               # Plotting the time series\n",
    "               plt.figure(figsize=(8, 4))\n",
    "               plt.plot(data)\n",
    "               plt.axis('off')  # Turn off axes for a clean image\n",
    "               plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "               plt.close()\n",
    "print(\"Time series generation completed. Time series are saved in the 'time_series_data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating images for remaining 4 classes since the kernel crashed in the previous cell after 14 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMA_1_without_trend: 4000 images\n",
      "ARMA_2_with_trend: 4000 images\n",
      "MA_3_with_trend: 4000 images\n",
      "ARMA_2_without_trend: 4000 images\n",
      "AR_3_without_trend: 4000 images\n",
      "MA_1_without_trend: 4000 images\n",
      "MA_1_with_trend: 4000 images\n",
      "AR_2_with_trend: 4000 images\n",
      "MA_2_without_trend: 4000 images\n",
      "AR_1_with_trend: 4000 images\n",
      "MA_2_with_trend: 4000 images\n",
      "MA_3_without_trend: 4000 images\n",
      "ARMA_3_with_trend: 4000 images\n",
      "AR_3_with_trend: 4000 images\n",
      "AR_2_without_trend: 4000 images\n",
      "ARMA_3_without_trend: 4000 images\n",
      "AR_1_without_trend: 4000 images\n",
      "ARMA_1_with_trend: 4000 images\n"
     ]
    }
   ],
   "source": [
    "# Adjusted classes for ARMA 2 and 3 with and without trend\n",
    "model_type = 'ARMA'\n",
    "orders = [2, 3]\n",
    "for order in orders:\n",
    "    for include_trend in [True, False]:\n",
    "        class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "        class_dir = os.path.join(output_dir, class_label)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for i in range(n_series):\n",
    "            data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "            # Plotting the time series\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(data)\n",
    "            plt.axis('off')  # Turn off axes for a clean image\n",
    "            plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "\n",
    "# Code to check the number of images in each folder\n",
    "folder_status = {}\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_status[folder_name] = len(os.listdir(folder_path))\n",
    "\n",
    "# Display folder status\n",
    "for class_label, image_count in folder_status.items():\n",
    "    print(f\"{class_label}: {image_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script loads and processes time series images for classification using deep learning. It resizes images, normalizes pixel values, and maps class labels. A subset of classes is selected, and the dataset is split into training, validation, and test sets using stratified sampling. A custom data generator is implemented to efficiently handle indexed batches for training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "image_size = (224, 224)  # Resize all images to this size\n",
    "\n",
    "\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")  # Path to your main directory with class subfolders\n",
    "\n",
    "\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])  # Filter directories only\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # Mapping class names to indices\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):  # Ensure it's an image file\n",
    "                img = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
    "                img = img.resize(image_size)  # Resize image\n",
    "                data.append(np.array(img))\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index mapping: {'ARMA_1_with_trend': 0, 'ARMA_1_without_trend': 1, 'ARMA_2_with_trend': 2, 'ARMA_2_without_trend': 3, 'ARMA_3_with_trend': 4, 'ARMA_3_without_trend': 5, 'AR_1_with_trend': 6, 'AR_1_without_trend': 7, 'AR_2_with_trend': 8, 'AR_2_without_trend': 9, 'AR_3_with_trend': 10, 'AR_3_without_trend': 11, 'MA_1_with_trend': 12, 'MA_1_without_trend': 13, 'MA_2_with_trend': 14, 'MA_2_without_trend': 15, 'MA_3_with_trend': 16, 'MA_3_without_trend': 17}\n"
     ]
    }
   ],
   "source": [
    "# Generate a mapping for classes\n",
    "classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # {'AR_1_with_trend': 0, 'MA_1_without_trend': 1, ...}\n",
    "print(f\"Class to index mapping: {class_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Classes: ['ARMA_1_with_trend', 'ARMA_1_without_trend', 'ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend', 'ARMA_3_without_trend', 'AR_1_with_trend', 'AR_1_without_trend', 'AR_2_with_trend', 'AR_2_without_trend', 'AR_3_with_trend', 'AR_3_without_trend', 'MA_1_with_trend', 'MA_1_without_trend', 'MA_2_with_trend', 'MA_2_without_trend', 'MA_3_with_trend', 'MA_3_without_trend']\n",
      "Number of classes: 18\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Print class information\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Number of classes: {len(classes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a subset of 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Subset of classes to start with (e.g., 8 representative classes)\n",
    "subset_classes = ['AR_1_with_trend', 'AR_2_without_trend', \n",
    "                  'MA_2_with_trend', 'MA_3_without_trend',\n",
    "                  'ARMA_1_with_trend']\n",
    "\n",
    "# Map subset classes to indices\n",
    "subset_indices = [class_to_idx[cls] for cls in subset_classes]\n",
    "\n",
    "# Filter data and labels for the subset\n",
    "subset_mask = np.isin(labels, subset_indices)\n",
    "data_subset = data[subset_mask]\n",
    "labels_subset = labels[subset_mask]\n",
    "\n",
    "# Reindex labels for the subset\n",
    "labels_subset = np.array([subset_indices.index(lbl) for lbl in labels_subset])\n",
    "\n",
    "\n",
    "data_subset = data_subset.astype('float32')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data subset shape: (20000, 224, 224, 3), size: 11484.38 MB\n",
      "Labels subset shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data subset shape: {data_subset.shape}, size: {data_subset.nbytes / (1024**2):.2f} MB\")\n",
    "print(f\"Labels subset shape: {labels_subset.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "# Split indices instead of the whole dataset\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(labels_subset)), test_size=0.3, stratify=labels_subset, random_state=42\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=1/3, stratify=labels_subset[temp_idx], random_state=42\n",
    ")\n",
    "\n",
    "# Custom DataGenerator that uses indices\n",
    "class IndexedDataGenerator(Sequence):\n",
    "    def __init__(self, data, labels, indices, batch_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_data = self.data[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "# Use indices to create separate generators\n",
    "train_generator = IndexedDataGenerator(data_subset, labels_subset, train_idx, batch_size=32)\n",
    "val_generator = IndexedDataGenerator(data_subset, labels_subset, val_idx, batch_size=32)\n",
    "test_generator = IndexedDataGenerator(data_subset, labels_subset, test_idx, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning and fine-tuning with ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script fine-tunes a ResNet50-based deep learning model for time series image classification. It first trains the model with frozen ResNet50 layers, adding custom classification layers on top. Transfer learning is applied using a low learning rate, and callbacks like early stopping, learning rate reduction, and model checkpointing are used. After initial training, the last 50 layers of ResNet50 are unfrozen for fine-tuning with an even lower learning rate to enhance performance while preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.3980 - loss: 1.1213\n",
      "Epoch 1: val_accuracy improved from -inf to 0.44025, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 941ms/step - accuracy: 0.3980 - loss: 1.1212 - val_accuracy: 0.4403 - val_loss: 0.9981 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709ms/step - accuracy: 0.4296 - loss: 0.9796\n",
      "Epoch 2: val_accuracy improved from 0.44025 to 0.46600, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 915ms/step - accuracy: 0.4296 - loss: 0.9796 - val_accuracy: 0.4660 - val_loss: 0.9305 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734ms/step - accuracy: 0.4342 - loss: 0.9566\n",
      "Epoch 3: val_accuracy improved from 0.46600 to 0.48050, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 951ms/step - accuracy: 0.4342 - loss: 0.9566 - val_accuracy: 0.4805 - val_loss: 0.9207 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719ms/step - accuracy: 0.4532 - loss: 0.9425\n",
      "Epoch 4: val_accuracy improved from 0.48050 to 0.49375, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 923ms/step - accuracy: 0.4532 - loss: 0.9426 - val_accuracy: 0.4938 - val_loss: 0.9144 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731ms/step - accuracy: 0.4488 - loss: 0.9389\n",
      "Epoch 5: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 936ms/step - accuracy: 0.4488 - loss: 0.9389 - val_accuracy: 0.4852 - val_loss: 0.9114 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717ms/step - accuracy: 0.4583 - loss: 0.9286\n",
      "Epoch 6: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 923ms/step - accuracy: 0.4583 - loss: 0.9286 - val_accuracy: 0.4800 - val_loss: 0.9088 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716ms/step - accuracy: 0.4654 - loss: 0.9246\n",
      "Epoch 7: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 914ms/step - accuracy: 0.4654 - loss: 0.9246 - val_accuracy: 0.4807 - val_loss: 0.9055 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708ms/step - accuracy: 0.4571 - loss: 0.9213\n",
      "Epoch 8: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 920ms/step - accuracy: 0.4571 - loss: 0.9212 - val_accuracy: 0.4855 - val_loss: 0.9016 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.4724 - loss: 0.9168\n",
      "Epoch 9: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 926ms/step - accuracy: 0.4724 - loss: 0.9168 - val_accuracy: 0.4793 - val_loss: 0.8939 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.4723 - loss: 0.9171\n",
      "Epoch 10: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 921ms/step - accuracy: 0.4723 - loss: 0.9171 - val_accuracy: 0.4908 - val_loss: 0.8949 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688ms/step - accuracy: 0.4822 - loss: 0.9001\n",
      "Epoch 11: val_accuracy improved from 0.49375 to 0.49725, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 883ms/step - accuracy: 0.4822 - loss: 0.9001 - val_accuracy: 0.4972 - val_loss: 0.8891 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - accuracy: 0.4790 - loss: 0.9055\n",
      "Epoch 12: val_accuracy improved from 0.49725 to 0.50325, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 875ms/step - accuracy: 0.4790 - loss: 0.9055 - val_accuracy: 0.5033 - val_loss: 0.8879 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679ms/step - accuracy: 0.4822 - loss: 0.9017\n",
      "Epoch 13: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 868ms/step - accuracy: 0.4822 - loss: 0.9017 - val_accuracy: 0.4930 - val_loss: 0.8851 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668ms/step - accuracy: 0.4957 - loss: 0.8895\n",
      "Epoch 14: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 856ms/step - accuracy: 0.4957 - loss: 0.8895 - val_accuracy: 0.4940 - val_loss: 0.8851 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668ms/step - accuracy: 0.4971 - loss: 0.8869\n",
      "Epoch 15: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 858ms/step - accuracy: 0.4971 - loss: 0.8869 - val_accuracy: 0.4978 - val_loss: 0.8875 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687ms/step - accuracy: 0.5066 - loss: 0.8845\n",
      "Epoch 16: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 876ms/step - accuracy: 0.5065 - loss: 0.8845 - val_accuracy: 0.4942 - val_loss: 0.8842 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664ms/step - accuracy: 0.5028 - loss: 0.8836\n",
      "Epoch 17: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 856ms/step - accuracy: 0.5028 - loss: 0.8837 - val_accuracy: 0.4990 - val_loss: 0.8790 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677ms/step - accuracy: 0.4885 - loss: 0.8938\n",
      "Epoch 18: val_accuracy improved from 0.50325 to 0.50525, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 871ms/step - accuracy: 0.4885 - loss: 0.8938 - val_accuracy: 0.5052 - val_loss: 0.8804 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682ms/step - accuracy: 0.5005 - loss: 0.8901\n",
      "Epoch 19: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 876ms/step - accuracy: 0.5005 - loss: 0.8901 - val_accuracy: 0.5035 - val_loss: 0.8724 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683ms/step - accuracy: 0.5067 - loss: 0.8813\n",
      "Epoch 20: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 877ms/step - accuracy: 0.5067 - loss: 0.8813 - val_accuracy: 0.5038 - val_loss: 0.8732 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Parameters\n",
    "IMAGE_SIZE = (224, 224, 3)  # Input size for ResNet50\n",
    "NUM_CLASSES = len(subset_classes)  # Number of classes in the subset\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "# Load the ResNet50 model with pretrained weights\n",
    "base_model = ResNet50(\n",
    "    input_shape=IMAGE_SIZE,\n",
    "    include_top=False,  # Exclude the final classification layers\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Pool the feature maps to a single vector\n",
    "x = BatchNormalization()(x)  # Normalize for stability\n",
    "x = Dense(512, activation=\"relu\")(x)  # Fully connected layer\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(NUM_CLASSES, activation=\"softmax\")(x)  # Final classification layer\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),  # Use a lower learning rate for transfer learning\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"subset_model.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4176 - loss: 1.0157\n",
      "Epoch 1: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 2s/step - accuracy: 0.4176 - loss: 1.0157 - val_accuracy: 0.4375 - val_loss: 0.9349 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4503 - loss: 0.9345\n",
      "Epoch 2: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 2s/step - accuracy: 0.4503 - loss: 0.9345 - val_accuracy: 0.4415 - val_loss: 0.9331 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4747 - loss: 0.9176\n",
      "Epoch 3: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.4746 - loss: 0.9176 - val_accuracy: 0.4600 - val_loss: 0.9086 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4951 - loss: 0.8946\n",
      "Epoch 4: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m859s\u001b[0m 2s/step - accuracy: 0.4951 - loss: 0.8947 - val_accuracy: 0.4462 - val_loss: 0.9176 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4909 - loss: 0.8817\n",
      "Epoch 5: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m860s\u001b[0m 2s/step - accuracy: 0.4909 - loss: 0.8817 - val_accuracy: 0.4405 - val_loss: 0.9327 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5009 - loss: 0.8775\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 2s/step - accuracy: 0.5009 - loss: 0.8775 - val_accuracy: 0.4180 - val_loss: 0.9657 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5171 - loss: 0.8507\n",
      "Epoch 7: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.5172 - loss: 0.8507 - val_accuracy: 0.4820 - val_loss: 0.8925 - learning_rate: 5.0000e-06\n",
      "Epoch 8/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5423 - loss: 0.8352\n",
      "Epoch 8: val_accuracy improved from 0.50525 to 0.50925, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.5422 - loss: 0.8352 - val_accuracy: 0.5092 - val_loss: 0.8573 - learning_rate: 5.0000e-06\n",
      "Epoch 9/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5521 - loss: 0.8245\n",
      "Epoch 9: val_accuracy improved from 0.50925 to 0.52225, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 2s/step - accuracy: 0.5521 - loss: 0.8245 - val_accuracy: 0.5222 - val_loss: 0.8633 - learning_rate: 5.0000e-06\n",
      "Epoch 10/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5683 - loss: 0.8043\n",
      "Epoch 10: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.5683 - loss: 0.8044 - val_accuracy: 0.5052 - val_loss: 0.8566 - learning_rate: 5.0000e-06\n",
      "Epoch 11/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5700 - loss: 0.7995\n",
      "Epoch 11: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.5700 - loss: 0.7995 - val_accuracy: 0.4940 - val_loss: 0.9390 - learning_rate: 5.0000e-06\n",
      "Epoch 12/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5836 - loss: 0.7823\n",
      "Epoch 12: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.5836 - loss: 0.7823 - val_accuracy: 0.5120 - val_loss: 0.8580 - learning_rate: 5.0000e-06\n",
      "Epoch 13/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6003 - loss: 0.7733\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m859s\u001b[0m 2s/step - accuracy: 0.6003 - loss: 0.7733 - val_accuracy: 0.5105 - val_loss: 0.9236 - learning_rate: 5.0000e-06\n",
      "Epoch 14/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6379 - loss: 0.7218\n",
      "Epoch 14: val_accuracy improved from 0.52225 to 0.52925, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.6379 - loss: 0.7218 - val_accuracy: 0.5293 - val_loss: 0.8920 - learning_rate: 2.5000e-06\n",
      "Epoch 15/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6464 - loss: 0.7027\n",
      "Epoch 15: val_accuracy did not improve from 0.52925\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.6464 - loss: 0.7027 - val_accuracy: 0.5217 - val_loss: 0.9021 - learning_rate: 2.5000e-06\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze some layers in the base model for fine-tuning\n",
    "for layer in base_model.layers[-50:]:  # Unfreeze the last 50 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_finetune = model.fit(\n",
    "    train_generator,  # Use the same training data\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,  # Fine-tune for fewer epochs to avoid overfitting\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental learning: dataset and model adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script defines functions for dynamically updating a dataset and adapting a deep learning model for incremental learning. It includes a custom data generator to efficiently load indexed batches and a function to update the dataset by selecting remembered and new classes while maintaining class balance. Additionally, a function is provided to update the model by freezing layers and adjusting the output layer for a new number of classes, ensuring a smooth adaptation to changing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator \n",
    "class IndexedDataGenerator(Sequence):\n",
    "    def __init__(self, data, labels, indices, batch_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_data = self.data[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the dataset \n",
    "def update_dataset(data, labels, class_to_idx, remembered_classes, new_classes, batch_size=32):\n",
    "    subset_classes = remembered_classes + new_classes\n",
    "    subset_indices = [class_to_idx[cls] for cls in subset_classes]\n",
    "    subset_mask = np.isin(labels, subset_indices)\n",
    "    data_subset = data[subset_mask]\n",
    "    labels_subset = labels[subset_mask]\n",
    "    labels_subset = np.array([subset_indices.index(lbl) for lbl in labels_subset])\n",
    "\n",
    "    # Split data into train, validation, and test indices\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        np.arange(len(labels_subset)), test_size=0.3, stratify=labels_subset, random_state=42\n",
    "    )\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx, test_size=1/3, stratify=labels_subset[temp_idx], random_state=42\n",
    "    )\n",
    "\n",
    "    # Create new generators\n",
    "    train_generator = IndexedDataGenerator(data_subset, labels_subset, train_idx, batch_size=batch_size)\n",
    "    val_generator = IndexedDataGenerator(data_subset, labels_subset, val_idx, batch_size=batch_size)\n",
    "    #test_generator = IndexedDataGenerator(data_subset, labels_subset, test_idx, batch_size=batch_size)\n",
    "\n",
    "    return train_generator, val_generator, data_subset, labels_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the model (freeze layers) \n",
    "def update_model_freeze(model, num_classes, learning_rate=1e-5):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False  # Freeze all layers\n",
    "\n",
    "    # Modify the output layer for the new number of classes\n",
    "    new_output = Dense(num_classes, activation=\"softmax\")(model.layers[-3].output)\n",
    "    updated_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "    # Recompile the model with a smaller learning rate\n",
    "    updated_model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding model with new classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script implements incremental learning across five increments by updating a pre-trained model with new classes while retaining knowledge from previous ones. It first loads a model trained on an initial subset of classes, then updates the dataset by selecting remembered and new classes at each step. The model is adjusted to accommodate the expanded class set while freezing earlier layers. Training is performed with callbacks for early stopping, learning rate reduction, and checkpointing to ensure stable learning and optimal performance throughout all five increments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# === Increment 1 Training ===\n",
    "\n",
    "# Load the pre-trained subset model\n",
    "model = load_model(\"subset_model.keras\")  # Model trained on the initial 5 classes\n",
    "\n",
    "# Define the current classes\n",
    "current_classes = [\n",
    "    'AR_1_with_trend', 'AR_2_without_trend',\n",
    "    'MA_2_with_trend', 'MA_3_without_trend',\n",
    "    'ARMA_1_with_trend'\n",
    "]\n",
    "\n",
    "# Define the first increment\n",
    "increment_1 = ['AR_1_without_trend', 'MA_1_with_trend', 'MA_1_without_trend']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 0.1804 - loss: 3.3579\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27437, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 464ms/step - accuracy: 0.1804 - loss: 3.3574 - val_accuracy: 0.2744 - val_loss: 1.9357 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.3006 - loss: 1.8195\n",
      "Epoch 2: val_accuracy improved from 0.27437 to 0.36021, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 454ms/step - accuracy: 0.3006 - loss: 1.8193 - val_accuracy: 0.3602 - val_loss: 1.3175 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.3423 - loss: 1.4226\n",
      "Epoch 3: val_accuracy improved from 0.36021 to 0.37708, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 458ms/step - accuracy: 0.3423 - loss: 1.4226 - val_accuracy: 0.3771 - val_loss: 1.1919 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.3683 - loss: 1.2987\n",
      "Epoch 4: val_accuracy improved from 0.37708 to 0.41104, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 457ms/step - accuracy: 0.3683 - loss: 1.2987 - val_accuracy: 0.4110 - val_loss: 1.1319 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - accuracy: 0.3869 - loss: 1.2405\n",
      "Epoch 5: val_accuracy improved from 0.41104 to 0.42625, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 453ms/step - accuracy: 0.3869 - loss: 1.2405 - val_accuracy: 0.4263 - val_loss: 1.0965 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.3973 - loss: 1.2108\n",
      "Epoch 6: val_accuracy improved from 0.42625 to 0.43875, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 457ms/step - accuracy: 0.3973 - loss: 1.2108 - val_accuracy: 0.4387 - val_loss: 1.0731 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4057 - loss: 1.1881\n",
      "Epoch 7: val_accuracy improved from 0.43875 to 0.44229, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 454ms/step - accuracy: 0.4057 - loss: 1.1881 - val_accuracy: 0.4423 - val_loss: 1.0559 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 0.4117 - loss: 1.1756\n",
      "Epoch 8: val_accuracy improved from 0.44229 to 0.44917, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 457ms/step - accuracy: 0.4117 - loss: 1.1756 - val_accuracy: 0.4492 - val_loss: 1.0419 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.4142 - loss: 1.1512\n",
      "Epoch 9: val_accuracy improved from 0.44917 to 0.45167, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 452ms/step - accuracy: 0.4142 - loss: 1.1512 - val_accuracy: 0.4517 - val_loss: 1.0346 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4166 - loss: 1.1371\n",
      "Epoch 10: val_accuracy improved from 0.45167 to 0.45562, saving model to final_8.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 454ms/step - accuracy: 0.4166 - loss: 1.1371 - val_accuracy: 0.4556 - val_loss: 1.0277 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "# === Increment 1 Training ===\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,  # Your dataset\n",
    "    labels=labels,  # Your dataset labels\n",
    "    class_to_idx=class_to_idx,  # Mapping from classes to indices\n",
    "    remembered_classes=current_classes[-3:],  # Remember only the last 3 classes\n",
    "    new_classes=increment_1,  # Add Increment 1 classes\n",
    "    batch_size=16\n",
    "    \n",
    ")\n",
    "\n",
    "# Update the model for Increment 1\n",
    "model = update_model_freeze(\n",
    "    model=model,\n",
    "    num_classes=len(current_classes) + len(increment_1),  # 3 remembered + 3 new classes = 6 total\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define callbacks for saving and early stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"final_8.keras\",  # Save the updated model as \"final_8.keras\"\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update `current_classes` to include the new ones\n",
    "current_classes = current_classes + increment_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# === Increment 2 Training ===\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model(\"final_8.keras\")  # Model trained on the initial 5 classes + increment 1\n",
    "\n",
    "# Define the current classes\n",
    "current_classes = [\n",
    "    'AR_1_with_trend', 'AR_2_without_trend',\n",
    "    'MA_2_with_trend', 'MA_3_without_trend',\n",
    "    'ARMA_1_with_trend', 'AR_1_without_trend', 'MA_1_with_trend',\n",
    "    'MA_1_without_trend'\n",
    "]\n",
    "\n",
    "# Define the second increment\n",
    "increment_2 = ['AR_2_with_trend', 'MA_2_without_trend', 'ARMA_1_without_trend']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.1514 - loss: 3.1123\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25896, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 459ms/step - accuracy: 0.1514 - loss: 3.1120 - val_accuracy: 0.2590 - val_loss: 1.7729 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.2526 - loss: 2.4462\n",
      "Epoch 2: val_accuracy improved from 0.25896 to 0.36583, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 457ms/step - accuracy: 0.2526 - loss: 2.4462 - val_accuracy: 0.3658 - val_loss: 1.5220 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.3209 - loss: 2.2210\n",
      "Epoch 3: val_accuracy improved from 0.36583 to 0.38458, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 461ms/step - accuracy: 0.3209 - loss: 2.2210 - val_accuracy: 0.3846 - val_loss: 1.4020 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.3464 - loss: 2.0575\n",
      "Epoch 4: val_accuracy improved from 0.38458 to 0.38917, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 479ms/step - accuracy: 0.3464 - loss: 2.0575 - val_accuracy: 0.3892 - val_loss: 1.3380 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 0.3705 - loss: 1.9475\n",
      "Epoch 5: val_accuracy improved from 0.38917 to 0.39542, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 495ms/step - accuracy: 0.3705 - loss: 1.9475 - val_accuracy: 0.3954 - val_loss: 1.2991 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - accuracy: 0.3589 - loss: 1.9098\n",
      "Epoch 6: val_accuracy improved from 0.39542 to 0.40125, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 499ms/step - accuracy: 0.3589 - loss: 1.9098 - val_accuracy: 0.4013 - val_loss: 1.2722 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - accuracy: 0.3783 - loss: 1.8545\n",
      "Epoch 7: val_accuracy improved from 0.40125 to 0.40458, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 490ms/step - accuracy: 0.3783 - loss: 1.8545 - val_accuracy: 0.4046 - val_loss: 1.2537 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step - accuracy: 0.3833 - loss: 1.8063\n",
      "Epoch 8: val_accuracy improved from 0.40458 to 0.40625, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 466ms/step - accuracy: 0.3833 - loss: 1.8064 - val_accuracy: 0.4062 - val_loss: 1.2401 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step - accuracy: 0.3899 - loss: 1.7641\n",
      "Epoch 9: val_accuracy did not improve from 0.40625\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 469ms/step - accuracy: 0.3899 - loss: 1.7641 - val_accuracy: 0.4029 - val_loss: 1.2306 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - accuracy: 0.3812 - loss: 1.7840\n",
      "Epoch 10: val_accuracy improved from 0.40625 to 0.40688, saving model to final_11.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 461ms/step - accuracy: 0.3812 - loss: 1.7839 - val_accuracy: 0.4069 - val_loss: 1.2209 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "# === Increment 2 Training ===\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,  # Your dataset\n",
    "    labels=labels,  # Your dataset labels\n",
    "    class_to_idx=class_to_idx,  # Mapping from classes to indices\n",
    "    remembered_classes=current_classes[-3:],  # Remember only the last 3 classes\n",
    "    new_classes=increment_2,  # Add Increment 2 classes\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Update the model for Increment 2\n",
    "model = update_model_freeze(\n",
    "    model=model,\n",
    "    num_classes=len(current_classes) + len(increment_2),  \n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define callbacks for saving and early stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"final_11.keras\",  # Save the updated model as \"final_11.keras\"\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update `current_classes` to include the new ones\n",
    "current_classes = current_classes + increment_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# === Increment 3 Training ===\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model(\"final_11.keras\")  \n",
    "\n",
    "# Define the current classes\n",
    "current_classes = [\n",
    "    'AR_1_with_trend', 'AR_2_without_trend',\n",
    "    'MA_2_with_trend', 'MA_3_without_trend',\n",
    "    'ARMA_1_with_trend', 'AR_1_without_trend', 'MA_1_with_trend',\n",
    "    'MA_1_without_trend', 'AR_2_with_trend', 'MA_2_without_trend',\n",
    "    'ARMA_1_without_trend'\n",
    "]\n",
    "\n",
    "# Define the second increment\n",
    "increment_3 = ['AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - accuracy: 0.1185 - loss: 2.7942\n",
      "Epoch 1: val_accuracy improved from -inf to 0.33021, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 466ms/step - accuracy: 0.1185 - loss: 2.7937 - val_accuracy: 0.3302 - val_loss: 1.6707 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.3424 - loss: 1.5886\n",
      "Epoch 2: val_accuracy improved from 0.33021 to 0.38708, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 492ms/step - accuracy: 0.3424 - loss: 1.5885 - val_accuracy: 0.3871 - val_loss: 1.3754 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.3777 - loss: 1.3550\n",
      "Epoch 3: val_accuracy improved from 0.38708 to 0.39479, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 491ms/step - accuracy: 0.3777 - loss: 1.3549 - val_accuracy: 0.3948 - val_loss: 1.2675 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.3869 - loss: 1.2667\n",
      "Epoch 4: val_accuracy improved from 0.39479 to 0.40458, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 490ms/step - accuracy: 0.3869 - loss: 1.2667 - val_accuracy: 0.4046 - val_loss: 1.2126 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step - accuracy: 0.3927 - loss: 1.2210\n",
      "Epoch 5: val_accuracy improved from 0.40458 to 0.41000, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 489ms/step - accuracy: 0.3927 - loss: 1.2210 - val_accuracy: 0.4100 - val_loss: 1.1801 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.3957 - loss: 1.1834\n",
      "Epoch 6: val_accuracy improved from 0.41000 to 0.41583, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 490ms/step - accuracy: 0.3957 - loss: 1.1834 - val_accuracy: 0.4158 - val_loss: 1.1581 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.4073 - loss: 1.1604\n",
      "Epoch 7: val_accuracy improved from 0.41583 to 0.42000, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 491ms/step - accuracy: 0.4073 - loss: 1.1604 - val_accuracy: 0.4200 - val_loss: 1.1425 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.4113 - loss: 1.1475\n",
      "Epoch 8: val_accuracy improved from 0.42000 to 0.42375, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 494ms/step - accuracy: 0.4113 - loss: 1.1475 - val_accuracy: 0.4238 - val_loss: 1.1310 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - accuracy: 0.4103 - loss: 1.1365\n",
      "Epoch 9: val_accuracy did not improve from 0.42375\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 463ms/step - accuracy: 0.4103 - loss: 1.1365 - val_accuracy: 0.4194 - val_loss: 1.1217 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - accuracy: 0.4200 - loss: 1.1218\n",
      "Epoch 10: val_accuracy did not improve from 0.42375\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 466ms/step - accuracy: 0.4200 - loss: 1.1218 - val_accuracy: 0.4219 - val_loss: 1.1144 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - accuracy: 0.4181 - loss: 1.1143\n",
      "Epoch 11: val_accuracy did not improve from 0.42375\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 464ms/step - accuracy: 0.4181 - loss: 1.1143 - val_accuracy: 0.4233 - val_loss: 1.1086 - learning_rate: 1.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - accuracy: 0.4189 - loss: 1.1078\n",
      "Epoch 12: val_accuracy improved from 0.42375 to 0.42625, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 462ms/step - accuracy: 0.4189 - loss: 1.1078 - val_accuracy: 0.4263 - val_loss: 1.1035 - learning_rate: 1.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.4196 - loss: 1.1131\n",
      "Epoch 13: val_accuracy did not improve from 0.42625\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 460ms/step - accuracy: 0.4197 - loss: 1.1131 - val_accuracy: 0.4248 - val_loss: 1.0991 - learning_rate: 1.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 0.4211 - loss: 1.0995\n",
      "Epoch 14: val_accuracy improved from 0.42625 to 0.42979, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 456ms/step - accuracy: 0.4211 - loss: 1.0995 - val_accuracy: 0.4298 - val_loss: 1.0951 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 0.4169 - loss: 1.1050\n",
      "Epoch 15: val_accuracy did not improve from 0.42979\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 454ms/step - accuracy: 0.4169 - loss: 1.1050 - val_accuracy: 0.4292 - val_loss: 1.0918 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.4185 - loss: 1.0957\n",
      "Epoch 16: val_accuracy did not improve from 0.42979\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 455ms/step - accuracy: 0.4185 - loss: 1.0956 - val_accuracy: 0.4294 - val_loss: 1.0892 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.4301 - loss: 1.0893\n",
      "Epoch 17: val_accuracy improved from 0.42979 to 0.43229, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 457ms/step - accuracy: 0.4301 - loss: 1.0893 - val_accuracy: 0.4323 - val_loss: 1.0863 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.4302 - loss: 1.0859\n",
      "Epoch 18: val_accuracy improved from 0.43229 to 0.43583, saving model to final_14.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 447ms/step - accuracy: 0.4302 - loss: 1.0859 - val_accuracy: 0.4358 - val_loss: 1.0837 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - accuracy: 0.4303 - loss: 1.0830\n",
      "Epoch 19: val_accuracy did not improve from 0.43583\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 463ms/step - accuracy: 0.4303 - loss: 1.0830 - val_accuracy: 0.4331 - val_loss: 1.0814 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 0.4351 - loss: 1.0767\n",
      "Epoch 20: val_accuracy did not improve from 0.43583\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 454ms/step - accuracy: 0.4351 - loss: 1.0767 - val_accuracy: 0.4346 - val_loss: 1.0795 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    }
   ],
   "source": [
    "# === Increment 3 Training ===\n",
    "\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,  # Your dataset\n",
    "    labels=labels,  # Your dataset labels\n",
    "    class_to_idx=class_to_idx,  # Mapping from classes to indices\n",
    "    remembered_classes=current_classes[-3:],  # Remember only the last 3 classes\n",
    "    new_classes=increment_3,  # Add Increment 3 classes\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Update the model for Increment 3\n",
    "model = update_model_freeze(\n",
    "    model=model,\n",
    "    num_classes=len(current_classes) + len(increment_3),  \n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define callbacks for saving and early stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"final_14.keras\",  # Save the updated model as \"final_14.keras\"\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update `current_classes` to include the new ones\n",
    "current_classes = current_classes + increment_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# === Increment 4 Training ===\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model(\"final_14.keras\")  \n",
    "\n",
    "# Define the current classes\n",
    "current_classes = [\n",
    "    'AR_1_with_trend', 'AR_2_without_trend',\n",
    "    'MA_2_with_trend', 'MA_3_without_trend',\n",
    "    'ARMA_1_with_trend', 'AR_1_without_trend', 'MA_1_with_trend',\n",
    "    'MA_1_without_trend', 'AR_2_with_trend', 'MA_2_without_trend',\n",
    "    'ARMA_1_without_trend', 'AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend'\n",
    "]\n",
    "\n",
    "# Define the second increment\n",
    "increment_4 = ['ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 0.2936 - loss: 2.0851\n",
      "Epoch 1: val_accuracy improved from -inf to 0.37625, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 456ms/step - accuracy: 0.2936 - loss: 2.0846 - val_accuracy: 0.3762 - val_loss: 1.2878 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.3721 - loss: 1.2682\n",
      "Epoch 2: val_accuracy improved from 0.37625 to 0.38292, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 457ms/step - accuracy: 0.3721 - loss: 1.2682 - val_accuracy: 0.3829 - val_loss: 1.2123 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 0.3835 - loss: 1.2041\n",
      "Epoch 3: val_accuracy improved from 0.38292 to 0.38604, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 458ms/step - accuracy: 0.3835 - loss: 1.2041 - val_accuracy: 0.3860 - val_loss: 1.1844 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - accuracy: 0.4039 - loss: 1.1716\n",
      "Epoch 4: val_accuracy improved from 0.38604 to 0.39625, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 462ms/step - accuracy: 0.4039 - loss: 1.1716 - val_accuracy: 0.3963 - val_loss: 1.1694 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.4006 - loss: 1.1611\n",
      "Epoch 5: val_accuracy did not improve from 0.39625\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 458ms/step - accuracy: 0.4006 - loss: 1.1611 - val_accuracy: 0.3917 - val_loss: 1.1575 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - accuracy: 0.4024 - loss: 1.1581\n",
      "Epoch 6: val_accuracy improved from 0.39625 to 0.39792, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 459ms/step - accuracy: 0.4024 - loss: 1.1581 - val_accuracy: 0.3979 - val_loss: 1.1490 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - accuracy: 0.4178 - loss: 1.1397\n",
      "Epoch 7: val_accuracy improved from 0.39792 to 0.40125, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 469ms/step - accuracy: 0.4178 - loss: 1.1397 - val_accuracy: 0.4013 - val_loss: 1.1415 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.4121 - loss: 1.1417\n",
      "Epoch 8: val_accuracy improved from 0.40125 to 0.40333, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 497ms/step - accuracy: 0.4121 - loss: 1.1417 - val_accuracy: 0.4033 - val_loss: 1.1370 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 0.4101 - loss: 1.1360\n",
      "Epoch 9: val_accuracy improved from 0.40333 to 0.40604, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 478ms/step - accuracy: 0.4101 - loss: 1.1360 - val_accuracy: 0.4060 - val_loss: 1.1342 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4174 - loss: 1.1264\n",
      "Epoch 10: val_accuracy improved from 0.40604 to 0.40646, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 455ms/step - accuracy: 0.4174 - loss: 1.1264 - val_accuracy: 0.4065 - val_loss: 1.1303 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.4228 - loss: 1.1264\n",
      "Epoch 11: val_accuracy improved from 0.40646 to 0.40729, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 460ms/step - accuracy: 0.4228 - loss: 1.1264 - val_accuracy: 0.4073 - val_loss: 1.1289 - learning_rate: 1.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.4294 - loss: 1.1192\n",
      "Epoch 12: val_accuracy improved from 0.40729 to 0.40792, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 456ms/step - accuracy: 0.4294 - loss: 1.1192 - val_accuracy: 0.4079 - val_loss: 1.1278 - learning_rate: 1.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4246 - loss: 1.1181\n",
      "Epoch 13: val_accuracy improved from 0.40792 to 0.41313, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 454ms/step - accuracy: 0.4246 - loss: 1.1181 - val_accuracy: 0.4131 - val_loss: 1.1252 - learning_rate: 1.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.4318 - loss: 1.1114\n",
      "Epoch 14: val_accuracy did not improve from 0.41313\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 459ms/step - accuracy: 0.4318 - loss: 1.1114 - val_accuracy: 0.4129 - val_loss: 1.1213 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 0.4326 - loss: 1.1063\n",
      "Epoch 15: val_accuracy did not improve from 0.41313\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 457ms/step - accuracy: 0.4326 - loss: 1.1063 - val_accuracy: 0.4125 - val_loss: 1.1211 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4352 - loss: 1.1058\n",
      "Epoch 16: val_accuracy improved from 0.41313 to 0.41833, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 454ms/step - accuracy: 0.4352 - loss: 1.1058 - val_accuracy: 0.4183 - val_loss: 1.1199 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.4337 - loss: 1.1044\n",
      "Epoch 17: val_accuracy improved from 0.41833 to 0.41854, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 456ms/step - accuracy: 0.4337 - loss: 1.1044 - val_accuracy: 0.4185 - val_loss: 1.1188 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.4332 - loss: 1.1031\n",
      "Epoch 18: val_accuracy improved from 0.41854 to 0.41938, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 458ms/step - accuracy: 0.4332 - loss: 1.1031 - val_accuracy: 0.4194 - val_loss: 1.1176 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 0.4419 - loss: 1.0977\n",
      "Epoch 19: val_accuracy improved from 0.41938 to 0.42167, saving model to final_17.keras\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 457ms/step - accuracy: 0.4419 - loss: 1.0977 - val_accuracy: 0.4217 - val_loss: 1.1156 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4310 - loss: 1.1007\n",
      "Epoch 20: val_accuracy did not improve from 0.42167\n",
      "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 455ms/step - accuracy: 0.4310 - loss: 1.1007 - val_accuracy: 0.4175 - val_loss: 1.1171 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    }
   ],
   "source": [
    "# === Increment 4 Training ===\n",
    "\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,  # Your dataset\n",
    "    labels=labels,  # Your dataset labels\n",
    "    class_to_idx=class_to_idx,  # Mapping from classes to indices\n",
    "    remembered_classes=current_classes[-3:],  # Remember only the last 3 classes\n",
    "    new_classes=increment_4,  # Add Increment 4 classes\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Update the model for Increment 4\n",
    "model = update_model_freeze(\n",
    "    model=model,\n",
    "    num_classes=len(current_classes) + len(increment_4),  \n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define callbacks for saving and early stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"final_17.keras\",  # Save the updated model as \"final_17.keras\"\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update `current_classes` to include the new ones\n",
    "current_classes = current_classes + increment_4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# === Increment 5 Training ===\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model(\"final_17.keras\")  \n",
    "\n",
    "# Define the current classes\n",
    "current_classes = [\n",
    "    'AR_1_with_trend', 'AR_2_without_trend',\n",
    "    'MA_2_with_trend', 'MA_3_without_trend',\n",
    "    'ARMA_1_with_trend', 'AR_1_without_trend', 'MA_1_with_trend',\n",
    "    'MA_1_without_trend', 'AR_2_with_trend', 'MA_2_without_trend',\n",
    "    'ARMA_1_without_trend', 'AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend',\n",
    "    'ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend'\n",
    "]\n",
    "\n",
    "# Define the second increment\n",
    "increment_5 = ['ARMA_3_without_trend' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.3069 - loss: 2.1210\n",
      "Epoch 1: val_accuracy improved from -inf to 0.51812, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 450ms/step - accuracy: 0.3071 - loss: 2.1201 - val_accuracy: 0.5181 - val_loss: 0.9034 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 0.5107 - loss: 0.8604\n",
      "Epoch 2: val_accuracy improved from 0.51812 to 0.53375, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 451ms/step - accuracy: 0.5107 - loss: 0.8604 - val_accuracy: 0.5337 - val_loss: 0.7766 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.5223 - loss: 0.7704\n",
      "Epoch 3: val_accuracy improved from 0.53375 to 0.54969, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 449ms/step - accuracy: 0.5223 - loss: 0.7704 - val_accuracy: 0.5497 - val_loss: 0.7404 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.5316 - loss: 0.7456\n",
      "Epoch 4: val_accuracy improved from 0.54969 to 0.55437, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 453ms/step - accuracy: 0.5316 - loss: 0.7456 - val_accuracy: 0.5544 - val_loss: 0.7237 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - accuracy: 0.5582 - loss: 0.7190\n",
      "Epoch 5: val_accuracy improved from 0.55437 to 0.56937, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 451ms/step - accuracy: 0.5582 - loss: 0.7190 - val_accuracy: 0.5694 - val_loss: 0.7112 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - accuracy: 0.5556 - loss: 0.7095\n",
      "Epoch 6: val_accuracy improved from 0.56937 to 0.57344, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 450ms/step - accuracy: 0.5556 - loss: 0.7095 - val_accuracy: 0.5734 - val_loss: 0.7024 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.5608 - loss: 0.7045\n",
      "Epoch 7: val_accuracy improved from 0.57344 to 0.58000, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 448ms/step - accuracy: 0.5608 - loss: 0.7045 - val_accuracy: 0.5800 - val_loss: 0.6968 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.5765 - loss: 0.6942\n",
      "Epoch 8: val_accuracy did not improve from 0.58000\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 448ms/step - accuracy: 0.5765 - loss: 0.6942 - val_accuracy: 0.5763 - val_loss: 0.6930 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.5768 - loss: 0.6926\n",
      "Epoch 9: val_accuracy improved from 0.58000 to 0.58312, saving model to final_18.keras\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 451ms/step - accuracy: 0.5768 - loss: 0.6926 - val_accuracy: 0.5831 - val_loss: 0.6883 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 0.5786 - loss: 0.6872\n",
      "Epoch 10: val_accuracy did not improve from 0.58312\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 457ms/step - accuracy: 0.5786 - loss: 0.6872 - val_accuracy: 0.5819 - val_loss: 0.6862 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "# === Increment 5 Training ===\n",
    "\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,  # Your dataset\n",
    "    labels=labels,  # Your dataset labels\n",
    "    class_to_idx=class_to_idx,  # Mapping from classes to indices\n",
    "    remembered_classes=current_classes[-3:],  # Remember only the last 3 classes\n",
    "    new_classes=increment_5,  # Add Increment 5 classes\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Update the model for Increment 5\n",
    "model = update_model_freeze(\n",
    "    model=model,\n",
    "    num_classes=len(current_classes) + len(increment_5),  \n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define callbacks for saving and early stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"final_18.keras\",  # Save the updated model as \"final_18.keras\"\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update `current_classes` to include the new ones\n",
    "current_classes = current_classes + increment_5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
